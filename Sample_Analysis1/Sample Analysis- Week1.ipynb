{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import botocore\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import seaborn as sns\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "import sys\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import corpora, models\n",
    "from gensim.corpora import Dictionary\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data \n",
    "\n",
    "Selecting the Top 500,000 based on Retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('April_Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset = \"Tweet\").reset_index(drop=True)\n",
    "df1 = df.sort_values(by ='Retweets',ascending=False)\n",
    "df1=df1.iloc[:500000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessesing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def find_retweeted(tweet):\n",
    "    '''This function will extract the twitter handles of retweed people'''\n",
    "    return re.findall('(?<=RT\\s)(@[A-Za-z]+[A-Za-z0-9-_]+)', tweet)\n",
    "\n",
    "def find_mentioned(tweet):\n",
    "    '''This function will extract the twitter handles of people mentioned in the tweet'''\n",
    "    return re.findall('(?<!RT\\s)(@[A-Za-z]+[A-Za-z0-9-_]+)', tweet)  \n",
    "\n",
    "def find_hashtags(tweet):\n",
    "    '''This function will extract hashtags'''\n",
    "    return re.findall('(#[A-Za-z]+[A-Za-z0-9-_]+)', tweet) \n",
    "def find_links(tweet):\n",
    "    return re.findall('(https?://\\S+)', tweet) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['retweeted'] = df1.Tweet.apply(find_retweeted)\n",
    "df1['mentioned'] = df1.Tweet.apply(find_mentioned)\n",
    "df1['hashtags'] = df1.Tweet.apply(find_hashtags)\n",
    "df1['links'] = df1.Tweet.apply(find_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'https?://\\S+', '', text) # Remove link\n",
    "    ext = re.sub(r'http?://\\S+', '', text)\n",
    "    text = re.sub(r'\\n',' ', text) # Remove line breaks\n",
    "    text = re.sub('\\s+', ' ', text).strip() # Remove leading, trailing, and extra spaces\n",
    "    text= re.sub('(?<=RT\\s)(@[A-Za-z]+[A-Za-z0-9-_]+)','',text).strip()\n",
    "    text= re.sub('(?<!RT\\s)(@[A-Za-z]+[A-Za-z0-9-_]+)','',text).strip()\n",
    "    text= re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)','',text).strip()\n",
    "    text= re.sub('([0-9]+)','',text).strip()\n",
    "    text= re.sub('(https?://\\S+)','',text).strip()\n",
    "    text= re.sub(r'RT',' ',text)\n",
    "    text= re.sub(r':',' ',text)\n",
    "    return text  \n",
    "df1['Tweet']= df1.Tweet.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(text):\n",
    "    return stemmer.stem(text);\n",
    "    \n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and (len(token) > 3):\n",
    "            result.append(stem(token))\n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation and Analysis on the Cleaned Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df1['Tweet'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2= pd.DataFrame(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = FreqDist(w for words in df2.Tweet for w in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 20 unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_word_freq = pd.DataFrame.from_dict(word_freq, orient='index', columns=['count'])\n",
    "top20w = df_word_freq.sort_values('count',ascending=False).head(20)\n",
    "plt.style.use('dark_background')\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(top20w['count'], top20w.index)\n",
    "plt.title('Top 20 words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 20 Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgdf_d = pd.DataFrame.from_dict(d_fq, orient='index', columns=['count'])\n",
    "bgdf_d.index = bgdf_d.index.map(lambda x: ' '.join(x))\n",
    "plt.figure(figsize=(16,7))\n",
    "plt.subplot(121)\n",
    "bgdf_d = bgdf_d.sort_values('count',ascending=False)\n",
    "sns.barplot(bgdf_d.head(20)['count'], bgdf_d.index[:20], color='pink')\n",
    "plt.title('Top bigrams in the tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and Visualising Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_loc = df1.Location.value_counts()\n",
    "top_loc = list(raw_loc[raw_loc>=10].index)\n",
    "top_only = df1[df1.Location.isin(top_loc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_loc(x):\n",
    "    if x == 'None':\n",
    "        return 'None'\n",
    "    elif x == 'Earth' or x =='Worldwide' or x == 'Everywhere':\n",
    "        return 'World'\n",
    "    elif 'New York' in x or 'NYC' in x:\n",
    "        return 'New York'    \n",
    "    elif 'London' in x:\n",
    "        return 'London'\n",
    "    elif 'Mumbai' in x:\n",
    "        return 'Mumbai'\n",
    "    elif 'Washington' in x and 'D' in x and 'C' in x:\n",
    "        return 'Washington DC'\n",
    "    elif 'San Francisco' in x:\n",
    "        return 'San Francisco'\n",
    "    elif 'Los Angeles' in x:\n",
    "        return 'Los Angeles'\n",
    "    elif 'Seattle' in x:\n",
    "        return 'Seattle'\n",
    "    elif 'Chicago' in x:\n",
    "        return 'Chicago'\n",
    "    elif 'Toronto' in x:\n",
    "        return 'Toronto'\n",
    "    elif 'Sacramento' in x:\n",
    "        return 'Sacramento'\n",
    "    elif 'Atlanta' in x:\n",
    "        return 'Atlanta'\n",
    "    elif 'California' in x:\n",
    "        return 'California'\n",
    "    elif 'Florida' in x:\n",
    "        return 'Florida'\n",
    "    elif 'Texas' in x:\n",
    "        return 'Texas'\n",
    "    elif 'United States' in x or 'USA' in x:\n",
    "        return 'USA'\n",
    "    elif 'United Kingdom' in x or 'UK' in x or 'Britain' in x:\n",
    "        return 'UK'\n",
    "    elif 'Canada' in x:\n",
    "        return 'Canada'\n",
    "    elif 'India' in x:\n",
    "        return 'India'\n",
    "    elif 'Kenya' in x:\n",
    "        return 'Kenya'\n",
    "    elif 'Nigeria' in x:\n",
    "        return 'Nigeria'\n",
    "    elif 'Australia' in x:\n",
    "        return 'Australia'\n",
    "    elif 'Indonesia' in x:\n",
    "        return 'Indonesia'\n",
    "    elif x in top_loc:\n",
    "        return x\n",
    "    \n",
    "df1['location_clean'] = df1['Location'].apply(lambda x: clean_loc(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "sns.countplot(y=df1.location_clean, order = df1.location_clean.value_counts().iloc[:25].index)\n",
    "plt.title('Top 25 locations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean= df1\n",
    "df_clean.drop_duplicates(subset='Tweet', inplace= True)\n",
    "data_text = df_clean[['Tweet']];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = data_text.astype('str');\n",
    "for idx in range(len(data_text)):\n",
    "    \n",
    "    #go through each word in each data_text row, remove stopwords, and set them on the index.\n",
    "    data_text.iloc[idx]['Tweet'] = [word for word in data_text.iloc[idx]['Tweet'].split(' ') if word not in stop_words];\n",
    "#     data_text.iloc[idx]['Tweet']= [word for word in data_text.iloc[idx]['Tweet'].split(' ') if len(word)>3];\n",
    "    \n",
    "    #print logs to monitor output\n",
    "    if idx % 1000 == 0:\n",
    "        sys.stdout.write('\\rc = ' + str(idx) + ' / ' + str(len(data_text)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [value[0] for value in data_text.iloc[0:].values];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [' '.join(sent) for sent in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilising IBM Watson for Topic Modelling on Top 10k Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df1.iloc[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Information Using Watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample(df, nums):\n",
    "    return df[nums:nums + 50]\n",
    "def perform(df_try):\n",
    "    data = pd.DataFrame(columns=[\"Tweet\", \"Language\", \"Sentiment\", \"Emotion\", \"Keyword\",\"Categories\"])\n",
    "    \n",
    "    for tweet in df_try[\"Tweet\"]:\n",
    "        try:\n",
    "            tw = tweet\n",
    "            response = service.analyze(text=tweet, features = Features(sentiment= DocumentSentimentResults(), emotion=EmotionOptions(), keywords=KeywordsOptions(), categories=CategoriesOptions())).get_result()\n",
    "        except:\n",
    "            print(\"Error in Tweet: \", tw)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            lan = response[\"language\"]\n",
    "            sent = response[\"sentiment\"][\"document\"][\"label\"]\n",
    "        except:\n",
    "            lan = 'en'\n",
    "            sent = 'neutral'\n",
    "            \n",
    "        ans = -1\n",
    "        place = -1\n",
    "        emotion = []\n",
    "        try:\n",
    "            for i in response[\"emotion\"][\"document\"][\"emotion\"]:\n",
    "                emotion.append(response[\"emotion\"][\"document\"][\"emotion\"][i])\n",
    "            for j in range(len(emotion)):\n",
    "                if emotion[j] > ans:\n",
    "                    ans  = emotion[j]\n",
    "                    place = j\n",
    "            if (place == 0):\n",
    "                emot = 'sadness'\n",
    "            elif (place == 1):\n",
    "                emot = 'joy'\n",
    "            elif (place == 2):\n",
    "                emot = 'fear'\n",
    "            elif (place == 3):\n",
    "                emot = 'disgust'\n",
    "            else:\n",
    "                emot = 'anger'\n",
    "            \n",
    "        except:\n",
    "            emot = \"sadness\"\n",
    "            \n",
    "        try:\n",
    "            word = response[\"keywords\"][0][\"text\"]\n",
    "        except:\n",
    "            word = '----'\n",
    "            \n",
    "        try:\n",
    "            cat = response['categories'][0]['label']\n",
    "        except:\n",
    "            cat = 'Unknown'\n",
    "            \n",
    "        final = {\"Tweet\":tw, \"Language\":lan, \"Sentiment\":sent, \"Emotion\":emot, \"Keyword\":word, \"Categories\":cat}\n",
    "        data = data.append(final, ignore_index=True)\n",
    "    return data\n",
    "\n",
    "def clean_simple(x):\n",
    "    a = []\n",
    "    for tw in x:\n",
    "        tw = tw.lower()\n",
    "        if (tw.startswith('rt') and tw.endswith('...')):\n",
    "            continue\n",
    "        if (tw.startswith('rt')):\n",
    "            tw = tw[2:]\n",
    "        a.append(' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",tw).split()))\n",
    "            \n",
    "    return set(a)\n",
    "\n",
    "def analyse_tweet(df):\n",
    "    df = df.drop_duplicates(subset = \"Tweet\")\n",
    "    df = df.sort_values(by = [\"Retweets\"], ascending=False)\n",
    "    df = df.reset_index(drop = True)\n",
    "    del df[\"Timestamp\"], df[\"Retweets\"], df[\"Location\"]\n",
    "    x = set(df[\"Tweet\"])\n",
    "    a = clean_simple(x)\n",
    "    a = list(a)\n",
    "    a = a[1:]\n",
    "    dff = pd.DataFrame(a, columns=[\"Tweet\"])\n",
    "    return dff\n",
    "\n",
    "def remove_short(df):\n",
    "    df.drop(df[df['Length'] < 30].index, inplace = True) \n",
    "    return df\n",
    "\n",
    "def extract_information(df):\n",
    "    print(\"Converting Data into suitable format...\")\n",
    "    df = analyse_tweet(df)\n",
    "    print(\"Data format complete!\")\n",
    "    md = pd.DataFrame(columns=[\"Tweet\", \"Language\", \"Sentiment\", \"Emotion\", \"Keyword\"])\n",
    "    nums = 0\n",
    "    while (nums < df.shape[0]):\n",
    "        print(\"Getting results:\", (nums/df.shape[0])*100, \"%...\")\n",
    "        df_mini = get_sample(df, nums)\n",
    "        df_mini = df_mini.reset_index(drop = True)\n",
    "        df_res = perform (df_mini)\n",
    "        md = pd.concat([md, df_res], axis=0)\n",
    "        nums = nums + 50\n",
    "    print(\"Analysis complete!\")\n",
    "    return md\n",
    "\n",
    "def extract_information(df):\n",
    "    print(\"Converting Data into suitable format...\")\n",
    "    df = analyse_tweet(df)\n",
    "    \n",
    "    print(\"Removing tweets with characters below 30.\")\n",
    "    lengths = []\n",
    "    for i in df[\"Tweet\"]:\n",
    "        l = len(i)\n",
    "        lengths.append(l)\n",
    "    df[\"Length\"] = lengths\n",
    "    df = remove_short(df)\n",
    "    del df[\"Length\"]\n",
    "    \n",
    "    print(\"Data format complete!\")\n",
    "    \n",
    "    md = pd.DataFrame(columns=[\"Tweet\", \"Language\", \"Sentiment\", \"Emotion\", \"Keyword\", \"Categories\"])\n",
    "    nums = 0\n",
    "    while (nums < df.shape[0]):\n",
    "        print(\"Getting results:\", (nums/df.shape[0])*100, \"%...\")\n",
    "        df_mini = get_sample(df, nums)\n",
    "        df_mini = df_mini.reset_index(drop = True)\n",
    "        df_res = perform (df_mini)\n",
    "        md = pd.concat([md, df_res], axis=0)\n",
    "        nums = nums + 50\n",
    "    print(\"Analysis complete!\")\n",
    "    return md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataframe):\n",
    "    size = dataframe.shape[0]\n",
    "    i = 0;\n",
    "    final = pd.DataFrame(columns=[\"Tweet\", \"Language\", \"Sentiment\", \"Emotion\", \"Keyword\", \"Categories\"])\n",
    "    while (i <= size):\n",
    "        print(\"TWEET CHUNK\", i, \"TO\", i+100)\n",
    "        res = extract_information(dataframe[i:i + 100])\n",
    "        i = i + 100\n",
    "        final = pd.concat([final, res], axis=0)\n",
    "        print()\n",
    "    final.reset_index(drop = True)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test= df1.iloc[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res= main(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.reset_index(drop = True)\n",
    "res.drop(res[res.Language!='en'].index, inplace=True)\n",
    "res = res.reset_index(drop = True)\n",
    "res.to_csv('5000_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly Saved a CSV file for the next 5000 Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation and Analysis on Watson Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('5000_1',names=['Temp','Tweet','Language','Sentiment','Emotion','Keyword','Categories'])\n",
    "df_2=pd.read_csv('5000_2',names=['Temp','Tweet','Language','Sentiment','Emotion','Keyword','Categories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('Temp',axis=1)\n",
    "df_2=df_2.drop('Temp',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df,df_2])\n",
    "df=df.drop([0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emotion=df.groupby(['Emotion']).count()\n",
    "df_emotion=df_emotion.sort_values(by='Tweet',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(df_emotion['Tweet'], df_emotion.index)\n",
    "plt.title('Emotion Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat=[]\n",
    "cat=[w.split('/') for w in df.Categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=[]\n",
    "for i in range(len(cat)):\n",
    "    categories.append(cat[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Main Categories']= categories\n",
    "df_cat=df.groupby(['Main Categories']).count()\n",
    "df_cat = df_cat.sort_values(by ='Tweet',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(df_cat['Tweet'], df_cat.index)\n",
    "plt.title('Topics discussed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_categories={}\n",
    "for i in range(len(cat)):\n",
    "    if(cat[i][1])=='health and fitness': \n",
    "        try:\n",
    "            ele=cat[i][2]\n",
    "            if ele in health_categories:\n",
    "                health_categories[ele]+=1\n",
    "            else:\n",
    "                health_categories[ele]=1\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_health = pd.DataFrame(health_categories.values(), health_categories.keys(),columns=['Count'])\n",
    "df_health= df_health.sort_values(by='Count',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(df_health['Count'], df_health.index)\n",
    "plt.title('Health Topics discussed')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
